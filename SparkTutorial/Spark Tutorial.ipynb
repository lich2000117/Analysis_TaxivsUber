{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "988f37e5",
   "metadata": {},
   "source": [
    "# Spark 3.0 Tutorial\n",
    "- Author: Akira Takihara Wang (https://github.com/akiratwang)\n",
    "- Tutorial Up-to-Date as of: April 2021  \n",
    "- Usage: For MAST30034 students only  \n",
    "\n",
    "Tutorial Operating System(s):\n",
    "- Windows 10 and WSL2\n",
    "- Linux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738ca586",
   "metadata": {},
   "source": [
    "# Working with Larger Datasets with a Scalable Solution!\n",
    "- Consider the full 2015 Taxi Dataset (at ~2GB per month @ ~24GB annually)\n",
    "- Datasets with more than 20k rows would be hard for Excel, but fine for Pandas.\n",
    "- A file with more than 100mil rows (a few GB) is large for Pandas.\n",
    "- Although `pandas` would be sufficient for each month, how about a whole year?\n",
    "\n",
    "That's right, use Spark 3.0!\n",
    "![image.png](https://spark.apache.org/images/spark-logo-trademark.png)\n",
    "\n",
    "\n",
    "## Pre-Requisites for this Tutorial\n",
    "1. You must already have Spark installed.\n",
    "2. You need the dataset downloaded.\n",
    "\n",
    "The code below downloads all 2015 data directly from the Amazon S3 Bucket. This is approximately ~21.3GB in size, so make sure you have ample storage space. You will only need to run this once.\n",
    "```python\n",
    "from os.path import getsize\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "output_dir = \"../data/large\"\n",
    "fname_template = \"yellow_tripdata_2015\"\n",
    "\n",
    "for m in range(1, 13):\n",
    "    month = str(m).zfill(2)\n",
    "    out = f'{fname_template}-{month}.csv'\n",
    "    url = f\"https://s3.amazonaws.com/nyc-tlc/trip+data/{out}\"\n",
    "    urlretrieve(url, f\"{output_dir}/{out}\")\n",
    "\n",
    "    print(f\"Done downloading {out} to {output_dir} with size {getsize(f'{output_dir}/{out}') / 1073741824:.2f}GB\")\n",
    "```\n",
    "\n",
    "## Optional Installation\n",
    "- Requires NodeJS and nbextensions installed:\n",
    "```bash\n",
    "# install NodeJS\n",
    "sudo apt install npm\n",
    "# install Jupyter Extensions\n",
    "pip3 install jupyter_contrib_nbextensions\n",
    "jupyter contrib nbextension install --user\n",
    "jupyter nbextension enable varInspector/main\n",
    "```\n",
    "- Follow instructions to install `SparkMonitor` (https://github.com/swan-cern/jupyter-extensions)\n",
    "```bash\n",
    "pip3 install sparkmonitor\n",
    "jupyter nbextension install sparkmonitor --py --user\n",
    "jupyter nbextension enable  sparkmonitor --py --user\n",
    "jupyter serverextension enable --py --system sparkmonitor  --user\n",
    "jupyter lab build\n",
    "ipython profile create\n",
    "echo \"c.InteractiveShellApp.extensions.append('sparkmonitor.kernelextension')\" >>  $(ipython profile locate default)/ipython_kernel_config.py\n",
    "```\n",
    "\n",
    "![image.gif](https://user-images.githubusercontent.com/6822941/29753710-ff8849b6-8b94-11e7-8f9c-bdc59bf72143.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1590e39a",
   "metadata": {},
   "source": [
    "Only run this cell below if you have installed `SparkMonitor`. Otherwise, it will result in an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cba90387",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:36:04.264725Z",
     "start_time": "2021-07-04T02:36:01.812051Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'swan_spark_conf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32me:\\Project\\mast30034_2021_s2_project_1-lich2000117\\SparkTutorial\\Spark Tutorial.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Project/mast30034_2021_s2_project_1-lich2000117/SparkTutorial/Spark%20Tutorial.ipynb#ch0000003?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m \u001b[39mimport\u001b[39;00m SparkContext\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Project/mast30034_2021_s2_project_1-lich2000117/SparkTutorial/Spark%20Tutorial.ipynb#ch0000003?line=2'>3</a>\u001b[0m \u001b[39m# Start the spark context\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Project/mast30034_2021_s2_project_1-lich2000117/SparkTutorial/Spark%20Tutorial.ipynb#ch0000003?line=3'>4</a>\u001b[0m sc \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39mgetOrCreate(conf\u001b[39m=\u001b[39mswan_spark_conf)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'swan_spark_conf' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Start the spark context\n",
    "sc = SparkContext.getOrCreate(conf=swan_spark_conf) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f39986",
   "metadata": {},
   "source": [
    "# Starting a Spark Session\n",
    "Start your Spark Session using `SparkSession.builder.getOrCreate()`. This is an object that provides some point of entry to interact with Spark functionalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d134d0c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:36:04.364719Z",
     "start_time": "2021-07-04T02:36:04.266228Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# create a spark session (which will run spark jobs)\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7f18f4",
   "metadata": {},
   "source": [
    "## Spark DataFrames\n",
    "To create a Spark DataFrames using a Pandas, simply pass it through `spark.createDataFrame()`.\n",
    "- It's common convention to name pandas df as `df` and spark df as `sdf`\n",
    "- And yes, Spark DataFrames *do* look ugly..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b0929f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:36:04.674751Z",
     "start_time": "2021-07-04T02:36:04.366245Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c153c222",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:36:13.056204Z",
     "start_time": "2021-07-04T02:36:04.675942Z"
    }
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o64.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (DESKTOP-FE732SE.modem executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:188)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:108)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:121)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:162)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:175)\r\n\t... 29 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3715)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2728)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2728)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2935)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:287)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:326)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:188)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:108)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:121)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:162)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:175)\r\n\t... 29 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32me:\\Project\\mast30034_2021_s2_project_1-lich2000117\\SparkTutorial\\Spark Tutorial.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Project/mast30034_2021_s2_project_1-lich2000117/SparkTutorial/Spark%20Tutorial.ipynb#ch0000008?line=0'>1</a>\u001b[0m sdf \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39mcreateDataFrame(df)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Project/mast30034_2021_s2_project_1-lich2000117/SparkTutorial/Spark%20Tutorial.ipynb#ch0000008?line=1'>2</a>\u001b[0m sdf\u001b[39m.\u001b[39;49mshow(\u001b[39m5\u001b[39;49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\dataframe.py:494\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/sql/dataframe.py?line=490'>491</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mParameter \u001b[39m\u001b[39m'\u001b[39m\u001b[39mvertical\u001b[39m\u001b[39m'\u001b[39m\u001b[39m must be a bool\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/sql/dataframe.py?line=492'>493</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(truncate, \u001b[39mbool\u001b[39m) \u001b[39mand\u001b[39;00m truncate:\n\u001b[1;32m--> <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/sql/dataframe.py?line=493'>494</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mshowString(n, \u001b[39m20\u001b[39;49m, vertical))\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/sql/dataframe.py?line=494'>495</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/sql/dataframe.py?line=495'>496</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/py4j/java_gateway.py?line=1314'>1315</a>\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/py4j/java_gateway.py?line=1315'>1316</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/py4j/java_gateway.py?line=1316'>1317</a>\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/py4j/java_gateway.py?line=1317'>1318</a>\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/py4j/java_gateway.py?line=1319'>1320</a>\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/py4j/java_gateway.py?line=1320'>1321</a>\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/py4j/java_gateway.py?line=1321'>1322</a>\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/py4j/java_gateway.py?line=1323'>1324</a>\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/py4j/java_gateway.py?line=1324'>1325</a>\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/sql/utils.py?line=108'>109</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/sql/utils.py?line=109'>110</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/sql/utils.py?line=110'>111</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/sql/utils.py?line=111'>112</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m py4j\u001b[39m.\u001b[39mprotocol\u001b[39m.\u001b[39mPy4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/sql/utils.py?line=112'>113</a>\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/py4j/protocol.py?line=323'>324</a>\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/py4j/protocol.py?line=324'>325</a>\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/py4j/protocol.py?line=325'>326</a>\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/py4j/protocol.py?line=326'>327</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/py4j/protocol.py?line=327'>328</a>\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/py4j/protocol.py?line=328'>329</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/py4j/protocol.py?line=329'>330</a>\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/py4j/protocol.py?line=330'>331</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/py4j/protocol.py?line=331'>332</a>\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o64.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (DESKTOP-FE732SE.modem executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:188)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:108)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:121)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:162)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:175)\r\n\t... 29 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3715)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2728)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2728)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2935)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:287)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:326)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:188)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:108)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:121)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:162)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:175)\r\n\t... 29 more\r\n"
     ]
    }
   ],
   "source": [
    "sdf = spark.createDataFrame(df)\n",
    "sdf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c52393a",
   "metadata": {},
   "source": [
    "- If you want to make it look nice (for the first 20 rows), then you can change the setting.\n",
    "- Use `sdf.limit()` as the alternative to `df.head()` from Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f96ad3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:36:13.060657Z",
     "start_time": "2021-07-04T02:36:13.057420Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e8e024",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:36:13.714685Z",
     "start_time": "2021-07-04T02:36:13.061787Z"
    }
   },
   "outputs": [],
   "source": [
    "sdf.limit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e365027",
   "metadata": {},
   "source": [
    "To convert a Spark DataFrame back into a Pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe4ff5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:36:15.000152Z",
     "start_time": "2021-07-04T02:36:13.716103Z"
    }
   },
   "outputs": [],
   "source": [
    "sdf.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1d3f3f",
   "metadata": {},
   "source": [
    "- Now, you might realize that this is still redundant as you need to read it in using Pandas with this method.\n",
    "- Likewise, using a `feather` dataset format requires you to read it into Pandas and then into Spark. \n",
    "\n",
    "## Overcoming Dataset Formats\n",
    "- Directly use Apache Arrow (framework that `feather` is built on) with `pip3 install pyarrow`\n",
    "- Set `spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0ca624",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:36:16.632164Z",
     "start_time": "2021-07-04T02:36:15.002780Z"
    }
   },
   "outputs": [],
   "source": [
    "# Benchmark Normal\n",
    "spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', False)\n",
    "%time sdf.toPandas()\n",
    "\n",
    "# Benchmark with Apache Arrow\n",
    "spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', True)\n",
    "%time sdf.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad1233b",
   "metadata": {},
   "source": [
    "As you can see, Apache Arrow is *magnitudes* faster!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191f72d6",
   "metadata": {},
   "source": [
    "## Reading in directly to Spark\n",
    "Use `spark.read`, where you can pass through either:\n",
    "- A single file;\n",
    "- comma separated file names;\n",
    "- or a folder directory with files.\n",
    "\n",
    "Below, we read all csv's in 2015 with a dataset size of 20GB+!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b04144",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:36:17.237183Z",
     "start_time": "2021-07-04T02:36:16.633821Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', True)\n",
    "\n",
    "sdf = spark.read.csv('../data/large', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47666edf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:36:17.687175Z",
     "start_time": "2021-07-04T02:36:17.238487Z"
    }
   },
   "outputs": [],
   "source": [
    "f\"{sdf.count():,} rows!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55efa05",
   "metadata": {},
   "source": [
    "Damn, ain't it great that you can read in all the csv's without having to append or merge them **AND** no `MemoryError`???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf4f768",
   "metadata": {},
   "source": [
    "## Schema\n",
    "- It is best practice to create a standard **schema** for your dataset. \n",
    "- It's very similar to creating a table in SQL (in fact its based on this) where you must specify what datatype the column is prior to adding data values.\n",
    "- View all data types here: https://spark.apache.org/docs/latest/sql-ref-datatypes.html\n",
    "\n",
    "\n",
    "Note: `RatecodeID` and `RateCodeID` are the same column, but inconsistent across months. We will be renaming it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2d5646",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:36:17.692663Z",
     "start_time": "2021-07-04T02:36:17.688536Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a2ccdf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:36:17.696669Z",
     "start_time": "2021-07-04T02:36:17.693787Z"
    }
   },
   "outputs": [],
   "source": [
    "ints = ('VendorID', 'passenger_count', 'RateCodeID', 'RatecodeID','payment_type')\n",
    "doubles = ('trip_distance', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude',\n",
    "           'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount')\n",
    "strings = ('store_and_fwd_flag',)\n",
    "dtimes = ('tpep_pickup_datetime', 'tpep_dropoff_datetime', )\n",
    "\n",
    "dtypes = {column: IntegerType() for column in ints}\n",
    "dtypes.update({column: DoubleType() for column in doubles})\n",
    "dtypes.update({column: StringType() for column in strings})\n",
    "dtypes.update({column: TimestampType() for column in dtimes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1510dc2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:36:17.700654Z",
     "start_time": "2021-07-04T02:36:17.697967Z"
    }
   },
   "outputs": [],
   "source": [
    "schema = StructType()\n",
    "\n",
    "for column in sdf.columns:\n",
    "    schema.add(column, # column name\n",
    "               dtypes[column], # data type\n",
    "               True # is nullable?\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3914b214",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:36:17.724644Z",
     "start_time": "2021-07-04T02:36:17.701758Z"
    }
   },
   "outputs": [],
   "source": [
    "sdf_with_schema = spark.read.csv('../data/large', header=True, schema=schema) \\\n",
    "    .withColumnRenamed(\"RatecodeID\",\"RateCodeID\") # rename the wrong column\n",
    "\n",
    "sdf_with_schema.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d10f716",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:36:17.728151Z",
     "start_time": "2021-07-04T02:36:17.725663Z"
    }
   },
   "outputs": [],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6088f72c",
   "metadata": {},
   "source": [
    "Although most PySpark operations will automatically handle incorrect data types, it is not recommended to rely on this from a Data Integrity standpoint. Schema's will be used by Business Analysts when describing or explaining the whole data pipeline. If the schema is incorrect or suspect to change, then many things can fall apart down the end of the pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e599e00d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:36:18.052156Z",
     "start_time": "2021-07-04T02:36:17.729986Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sdf_with_schema.limit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace2abbe",
   "metadata": {},
   "source": [
    "## Transformations and Lazy Evaluation (IMPORTANT)\n",
    "- Transformations transform a Spark DataFrame into a new DataFrame *without* altering the original data, making Spark **immutable**.\n",
    "- For example, operations will return transformed results rather than mutating the original. \n",
    "- It's common to see `sdf = sdf.some_transformation()` if you are looking to overwrite it.\n",
    "- Finally, all operations in Spark are evaluated lazily! That is, the data doesn't \"move\" until called upon.\n",
    "\n",
    "Take for example the code block below. Even when renaming columns, we need to overwrite the original variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115687f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:36:18.065680Z",
     "start_time": "2021-07-04T02:36:18.053343Z"
    }
   },
   "outputs": [],
   "source": [
    "sdf_with_schema = sdf_with_schema.withColumnRenamed('tpep_pickup_datetime', 'pickup_time') \\\n",
    "    .withColumnRenamed('tpep_dropoff_datetime', 'dropoff_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f836b70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:36:18.345165Z",
     "start_time": "2021-07-04T02:36:18.066910Z"
    }
   },
   "outputs": [],
   "source": [
    "sdf_with_schema.limit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebf5d93",
   "metadata": {},
   "source": [
    "## DataType Conversions\n",
    "Consider `tpep_pickup_datetime` as a `StringType()` and wish to convert to a `TimeStampType()`:\n",
    "```python\n",
    "# Method 1 using withColumn()\n",
    "sdf_with_schema.withColumn(\"tpep_pickup_datetime\", col(\"tpep_pickup_datetime\").cast(TimestampType()))\n",
    "\n",
    "# Method 2 using select()\n",
    "sdf_with_schema.select(col(\"tpep_pickup_datetime\").cast(\"Timestamp\"))\n",
    "\n",
    "# Method 3 using selectExpr() - similar to SQL syntax\n",
    "sdf_with_schema.selectExpr(\"cast(tpep_pickup_datetime as timestamp)\")\n",
    "```\n",
    "\n",
    "For an actual example, let's take a look at the `store_and_fwd_flag` which should be boolean. Currently, we have `N` and `Y` which we can resolve by assigning the column to a boolean condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d686af38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:36:18.497684Z",
     "start_time": "2021-07-04T02:36:18.346646Z"
    }
   },
   "outputs": [],
   "source": [
    "sdf_with_schema_temp = sdf_with_schema.withColumn(\"store_and_fwd_flag_bool\", \n",
    "                                             (sdf_with_schema[\"store_and_fwd_flag\"] == 'Y') \\\n",
    "                                             .cast(\"boolean\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec8bdba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:36:18.711660Z",
     "start_time": "2021-07-04T02:36:18.498976Z"
    }
   },
   "outputs": [],
   "source": [
    "sdf_with_schema_temp.select('store_and_fwd_flag', 'store_and_fwd_flag_bool').limit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa2ab93",
   "metadata": {},
   "source": [
    "As you can see, we now have the string `store_and_fwd_flag` set to boolean now under `store_and_fwd_flag_bool`. Let's change it in the `sdf_with_schema` DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6eabf4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:36:18.723681Z",
     "start_time": "2021-07-04T02:36:18.712804Z"
    }
   },
   "outputs": [],
   "source": [
    "sdf_with_schema = sdf_with_schema.withColumn(\"store_and_fwd_flag\", \n",
    "                                             (sdf_with_schema[\"store_and_fwd_flag\"] == 'Y') \\\n",
    "                                             .cast(\"boolean\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba271e4",
   "metadata": {},
   "source": [
    "## Retrieving and Filtering Data \n",
    "Collecting:\n",
    "- The `collect()` method is an operation that _collects_ all the rows for you (recall that Spark has lazy evaluation, so this method is the evaluation step).\n",
    "- If you use `collect()` on the full dataset or large partition, you will still result in an `OutOfMemoryError` as it will need to bring it into memory.\n",
    "- If you want to just get the size of the result, you can use `count()`.\n",
    "\n",
    "Filtering:\n",
    "- Similar to the syntax with `df.loc[]` from Pandas.\n",
    "- Use bitwise `&` or `|` to filter based on several conditions.\n",
    "- If you want to use NumPy's `.isin()` method, it's the same for Spark (and bitwise not `~` for the not in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0ebdc6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:36:18.729677Z",
     "start_time": "2021-07-04T02:36:18.725116Z"
    }
   },
   "outputs": [],
   "source": [
    "small_sdf = sdf_with_schema.limit(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6d6d29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:36:18.788204Z",
     "start_time": "2021-07-04T02:36:18.735263Z"
    }
   },
   "outputs": [],
   "source": [
    "rows = small_sdf.select('total_amount').limit(5).collect()\n",
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04d993a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:36:18.793665Z",
     "start_time": "2021-07-04T02:36:18.790539Z"
    }
   },
   "outputs": [],
   "source": [
    "# you can index your rows like normal lists\n",
    "rows[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74e8703",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:36:19.396158Z",
     "start_time": "2021-07-04T02:36:18.794911Z"
    }
   },
   "outputs": [],
   "source": [
    "small_sdf.filter(small_sdf.store_and_fwd_flag == True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa07acd",
   "metadata": {},
   "source": [
    "You can also filter DataFrame rows using `startswith()`, `endswith()`, and `contains()` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e064adc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:36:19.797661Z",
     "start_time": "2021-07-04T02:36:19.397411Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# all trips whose pickups are -74.X, 40.Y\n",
    "small_sdf.filter((small_sdf.pickup_longitude.startswith('-74.')) \n",
    "                 & (small_sdf.pickup_latitude.startswith('40.')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564f116b",
   "metadata": {},
   "source": [
    "Even better, you can use the SQL `LIKE` syntax!\n",
    "- `like()` for the SQL `LIKE`;\n",
    "- and `rlike()` for regex matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065de664",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:36:20.171167Z",
     "start_time": "2021-07-04T02:36:19.799177Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# using SQL LIKE\n",
    "small_sdf.filter(small_sdf.pickup_time.like('%19:%%:%%'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef179205",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:36:20.533156Z",
     "start_time": "2021-07-04T02:36:20.172618Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# using regex\n",
    "small_sdf.filter(small_sdf.pickup_time.rlike(r'.+\\s(19):\\d{2}:\\d{2}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a7b2be",
   "metadata": {},
   "source": [
    "## Unique Values, Duplicates, and Missing Values\n",
    "- You can easily grab unique values using `sdf.distinct()` and drop duplicates using `sdf.dropDuplicates()`.\n",
    "- For missing values, it's the same with Pandas `.fillna()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c34e9a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:36:20.797161Z",
     "start_time": "2021-07-04T02:36:20.534313Z"
    }
   },
   "outputs": [],
   "source": [
    "small_sdf.select('passenger_count').distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413ac2ea",
   "metadata": {},
   "source": [
    "Here's a code snippet which inserts a `null` value into `total_amount`, then finds the number `nulls` present in `total_amount`. This is because the dataset has no nulls present.\n",
    "- We should get 1 instance back as there are no `nulls` except the one we inserted.\n",
    "- As you can see, granular changes with Spark requires a round-a-bout approach!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550b6601",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:36:21.506679Z",
     "start_time": "2021-07-04T02:36:20.798373Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a single row aand convert to sdf\n",
    "temp = small_sdf.limit(1).toPandas()\n",
    "temp.rename({'pickup_time': 'tpep_pickup_datetime', 'dropoff_time': 'tpep_dropoff_datetime'}, axis=1, inplace=True)\n",
    "temp['total_amount'] = None\n",
    "r = spark.createDataFrame(temp, schema)\n",
    "r = r.withColumn(\"store_and_fwd_flag\",(r[\"store_and_fwd_flag\"] == 'true').cast(\"boolean\"))\n",
    "\n",
    "# take the union of the two (that is, add the row to small_sdf)\n",
    "small_sdf = small_sdf.union(r)\n",
    "\n",
    "# sample 5 random rows from 10% of all the data\n",
    "small_sdf.sample(0.1).limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b79d6a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:36:21.966160Z",
     "start_time": "2021-07-04T02:36:21.507918Z"
    }
   },
   "outputs": [],
   "source": [
    "# now, find all row values that are nan or null for total_amount\n",
    "small_sdf.where(col(\"total_amount\").isNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2bcb79",
   "metadata": {},
   "source": [
    "## Sorting Values\n",
    "- As you should know from 2nd year Algorithms, sorting algorithms whilst efficient, are still costly!\n",
    "- Even for a fast `O(nlogn)` sorting algorithm, our dataset will require `146,000,000 x log2(146,000,000) = 146,000,000 x 27.12` operations!\n",
    "- So, be careful when you run a sort - avoid it if you can.\n",
    "\n",
    "Here's a few ways to sort a column:\n",
    "```python\n",
    "# sort by total amount from largest and passenger count from smallest\n",
    "small_sdf.sort(small_sdf.total_amount.desc(), small_sdf.passenger_count.asc())\n",
    "small_sdf.sort(col(\"total_amount\").desc(), col(\"passenger_count\").asc())\n",
    "small_sdf.orderBy(col(\"total_amount\").desc(), col(\"passenger_count\").asc())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0451bda6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:36:22.413160Z",
     "start_time": "2021-07-04T02:36:21.967343Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "small_sdf.sort(col(\"total_amount\").desc(), col(\"passenger_count\").asc())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d90ceb7",
   "metadata": {},
   "source": [
    "## Aggregations\n",
    "- Aggregations are always a useful function whether it be for summarising data for analysis or fact tables.\n",
    "- Like Pandas, Spark also covers `count()`, `mean()`, `max()`, `agg()`, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c448c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:36:23.508656Z",
     "start_time": "2021-07-04T02:36:22.414446Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "\n",
    "small_sdf.groupBy(\"passenger_count\") \\\n",
    "    .agg(mean(\"total_amount\").alias(\"Average Trip Amount USD$\"),\n",
    "         mean(\"trip_distance\").alias(\"Average Distance in Miles\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbbfdda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:36:25.750652Z",
     "start_time": "2021-07-04T02:36:23.509933Z"
    }
   },
   "outputs": [],
   "source": [
    "# and yes, it does work on the full dataset (albeit it does take time...)\n",
    "results = sdf_with_schema.groupBy(\"passenger_count\") \\\n",
    "    .agg(mean(\"total_amount\").alias('avg_trip_amount')) \\\n",
    "    .orderBy(\"passenger_count\")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc04543a",
   "metadata": {},
   "source": [
    "## Writing to Disk\n",
    "- Conventionally, the \"go-to\" dataset was a `csv`. For good reasons, we have explored alternatives such as data serialization methods with formats such as Python's `pickle` or Apache Arrow's `feather`.\n",
    "- Spark introduces its' own type which is a **Parquet File**.\n",
    "- You can write a specific format (if supported) using `sdf.write.format(\"parquet\").save(path)`.\n",
    "\n",
    "### Parquet:\n",
    "- Parquet files are stored as a directory structure which contains data files, metadata, and some compressed files.|\n",
    "- If the file already exists, it cannot be overwritten without removing the existing file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db41e9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:42:06.345914Z",
     "start_time": "2021-07-04T02:42:06.340030Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check to see if the fpath already exists. If so, remove it.\n",
    "from shutil import rmtree\n",
    "from os import path\n",
    "\n",
    "fpath = '../data/aggregated_results.parquet/'\n",
    "if path.exists(fpath):\n",
    "    rmtree(fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b43697",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:43:04.571667Z",
     "start_time": "2021-07-04T02:43:02.752967Z"
    }
   },
   "outputs": [],
   "source": [
    "results.write.format('parquet').save('../data/aggregated_results.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8805b21e",
   "metadata": {},
   "source": [
    "Reading in `parquet` files are similar to `csv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129561d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:43:14.596285Z",
     "start_time": "2021-07-04T02:43:14.343633Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.read.parquet(fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267c2b42",
   "metadata": {},
   "source": [
    "## Union and Merging\n",
    "- The `union()` method merges two Spark DataFrames and returns *a new* DataFrame with all rows from the two DataFrames *including duplicates*. \n",
    "- It works identical to SQL `UNION` and as a result, may include duplicate results.\n",
    "- If you want no duplicates, you can do `union().distinct()` (distinct was mentioned previously).\n",
    "\n",
    "The example below takes the union of two identical DataFrames consisting of 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fac8d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:45:08.347850Z",
     "start_time": "2021-07-04T02:45:08.154880Z"
    }
   },
   "outputs": [],
   "source": [
    "sdf1 = spark.createDataFrame(df.iloc[:5])\n",
    "sdf2 = spark.createDataFrame(df.iloc[:5])\n",
    "\n",
    "sdf1.union(sdf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21596b5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:45:22.726981Z",
     "start_time": "2021-07-04T02:45:21.883780Z"
    }
   },
   "outputs": [],
   "source": [
    "sdf1.union(sdf2).distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abd056f",
   "metadata": {},
   "source": [
    "# User Defined Functions (UDF) and Pandas UDFs\n",
    "So far, all the functions and methods have been about simple aggregations or filtering rows. However, preprocessing and data cleansing usually requires more powerful tools such as `regex`.\n",
    "\n",
    "Unlike Pandas's `apply()` method (and also `rdd.map()`), we need to do a \"bit\" more work to generate UDFs.\n",
    "\n",
    "1. Create a function with a `@udf()` decorator.\n",
    "2. Specify an output data type (i.e `StringType()`) as format `@udf(\"string\")` or `@udf(StringType())`.\n",
    "3. Apply onto column(s) of choice (remembering that Spark is immutable).\n",
    "\n",
    "Alternatively, if we want to use Pandas framework:\n",
    "1. Create a function with a `@pandas_udf()` decorator and format as required.\n",
    "2. Apply onto column(s) of choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cfe392",
   "metadata": {},
   "source": [
    "In the following example, we will create a tuple consisting of pickup lat/lon to 4 decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3808d483",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:46:17.893091Z",
     "start_time": "2021-07-04T02:46:17.890608Z"
    }
   },
   "outputs": [],
   "source": [
    "# using UDF\n",
    "@F.udf(ArrayType(DoubleType(), True))\n",
    "def create_coords(lat, lon):\n",
    "    return round(lat, 4), round(lon, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522148b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:46:19.247612Z",
     "start_time": "2021-07-04T02:46:18.262554Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "small_sdf.withColumn(\"pickup_coords\", create_coords(col(\"pickup_latitude\"), col(\"pickup_longitude\"))) \\\n",
    "    .limit(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9276beec",
   "metadata": {},
   "source": [
    "And here's an example of mapping values from our data dictionary using a Pandas UDF:\n",
    "- Type definition Syntax: https://www.python.org/dev/peps/pep-0484/#type-definition-syntax\n",
    "- Function Decorators: https://johnpaton.net/posts/clean-spark-udfs/\n",
    "\n",
    "The Pandas UDF is also quite new so there isn't much *help* other than the documentation: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.pandas_udf.html?highlight=pandas%20udf\n",
    "\n",
    "Syntax:\n",
    "```python\n",
    "@pandas_udf(THE DATATYPE OF THE OUTPUT)\n",
    "def FUNCTION_NAME(ARGUMENTS: INPUT DATA FORMAT) -> OUTPUT DATA FORMAT:\n",
    "    ...\n",
    "    return ...\n",
    "\n",
    "sdf.withColumn(COLUMN OUT, FUNCTION_NAME(col(COLUMN IN)))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcec5375",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:46:22.401172Z",
     "start_time": "2021-07-04T02:46:22.398536Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032ccb16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:46:22.608592Z",
     "start_time": "2021-07-04T02:46:22.587296Z"
    }
   },
   "outputs": [],
   "source": [
    "vendors = {1: 'Creative Mobile Technologies, LLC', 2: 'VeriFone Inc.'}\n",
    "\n",
    "@pandas_udf(\"string\")\n",
    "def vendorMap(vid_col: pd.Series) -> pd.Series:\n",
    "    return vid_col.map(vendors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a360d69c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:46:23.891004Z",
     "start_time": "2021-07-04T02:46:22.778917Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "small_sdf.withColumn(\"VendorName\", vendorMap(col(\"VendorID\"))) \\\n",
    "    .limit(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de8ffa1",
   "metadata": {},
   "source": [
    "And that's the basics of PySpark! If you would like to further increase your scope, here are some pathways:\n",
    "- Data Science: Continue with Spark's MLlib to perform machine learning.\n",
    "- Data Engineering: Learn Spark SQL and Spark Connectors (i.e connecting to data sources such as S3 buckets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
