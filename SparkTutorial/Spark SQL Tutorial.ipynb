{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "988f37e5",
   "metadata": {},
   "source": [
    "# Spark SQL Tutorial\n",
    "- Author: Akira Takihara Wang (https://github.com/akiratwang)\n",
    "- Tutorial Up-to-Date as of: April 2021  \n",
    "- Usage: For MAST30034 students only  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8cb0abc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:55:27.177141Z",
     "start_time": "2021-07-04T02:55:24.102415Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'swan_spark_conf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32me:\\Project\\mast30034_2021_s2_project_1-lich2000117\\SparkTutorial\\Spark SQL Tutorial.ipynb Cell 2'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Project/mast30034_2021_s2_project_1-lich2000117/SparkTutorial/Spark%20SQL%20Tutorial.ipynb#ch0000001?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m \u001b[39mimport\u001b[39;00m SparkSession\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Project/mast30034_2021_s2_project_1-lich2000117/SparkTutorial/Spark%20SQL%20Tutorial.ipynb#ch0000001?line=3'>4</a>\u001b[0m \u001b[39m# Start the spark context\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Project/mast30034_2021_s2_project_1-lich2000117/SparkTutorial/Spark%20SQL%20Tutorial.ipynb#ch0000001?line=4'>5</a>\u001b[0m sc \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39mgetOrCreate(conf\u001b[39m=\u001b[39mswan_spark_conf)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Project/mast30034_2021_s2_project_1-lich2000117/SparkTutorial/Spark%20SQL%20Tutorial.ipynb#ch0000001?line=6'>7</a>\u001b[0m \u001b[39m# create a spark session (which will run spark jobs)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Project/mast30034_2021_s2_project_1-lich2000117/SparkTutorial/Spark%20SQL%20Tutorial.ipynb#ch0000001?line=7'>8</a>\u001b[0m spark \u001b[39m=\u001b[39m SparkSession\u001b[39m.\u001b[39mbuilder\u001b[39m.\u001b[39mgetOrCreate()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'swan_spark_conf' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Start the spark context\n",
    "sc = SparkContext.getOrCreate(conf=swan_spark_conf)\n",
    "\n",
    "# create a spark session (which will run spark jobs)\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# apply settings to session\n",
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)\n",
    "spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a7901b",
   "metadata": {},
   "source": [
    "Previously, we saw how to create a `schema` using PySpark code. If you are more familiar with SQL, then you may want to use a Data Definition Language (DDL) string to create a schema. \n",
    "\n",
    "Syntax:\n",
    "```python\n",
    "ddl = \"\"\"\n",
    "`COLUMN 1` DATATYPE1, `COLUMN 2` DATATYPE2, ...\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "Although it may make sense to pass through the datetimes as type `TIMESTAMP`, the format may be inconsistent. As an example, we will assume the worst case and \"manually\" fix it using a function for your benefit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c1f593",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:56:11.106689Z",
     "start_time": "2021-07-04T02:56:11.104749Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a DDL\n",
    "schema = \"\"\"\n",
    "`VendorID` INT, `tpep_pickup_datetime` STRING, `tpep_dropoff_datetime` STRING,\n",
    "`passenger_count` INT, `trip_distance` DOUBLE, `pickup_longitude` DOUBLE, `pickup_latitude` DOUBLE,\n",
    "`RateCodeID` INT, `store_and_fwd_flag` STRING, `dropoff_longitude` DOUBLE, `dropoff_latitude` DOUBLE,\n",
    "`payment_type` INT, `fare_amount` DOUBLE, `extra` DOUBLE, `mta_tax` DOUBLE, `tip_amount` DOUBLE,\n",
    "`tolls_amount` DOUBLE, `improvement_surcharge` DOUBLE, `total_amount` DOUBLE\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fa2391",
   "metadata": {},
   "source": [
    "Like PySpark, you can simply pass it through the `schema` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ab6b21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:56:11.565803Z",
     "start_time": "2021-07-04T02:56:11.549782Z"
    }
   },
   "outputs": [],
   "source": [
    "sdf = spark.read.csv('../data/sample.csv', header=True, schema=schema)\n",
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d7ea7e",
   "metadata": {},
   "source": [
    "- As you can see, this achieves the same result as using `StructType()`, but may be easier or more difficult depending on the number of columns you have. \n",
    "- My personal preference would be using `StructType()` for this dataset as you can use generator functions to simplify the allocation of dtypes.\n",
    "- However, depending on your role or project scope, you may already have an existing DDL you can use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6754d319",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:56:13.656078Z",
     "start_time": "2021-07-04T02:56:13.433186Z"
    }
   },
   "outputs": [],
   "source": [
    "sdf.limit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fa9980",
   "metadata": {},
   "source": [
    "Now let's get our datetimes into `TIMESTAMP` formats (since our current dataset does not have the correct format)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cb9a82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:55:36.761319Z",
     "start_time": "2021-07-04T02:55:36.754876Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d070f925",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:55:43.564829Z",
     "start_time": "2021-07-04T02:55:43.544733Z"
    }
   },
   "outputs": [],
   "source": [
    "# create UDF\n",
    "from datetime import datetime\n",
    "\n",
    "@F.udf(\"timestamp\")\n",
    "def format_dtime(dtime):\n",
    "    date, time = dtime.split()\n",
    "    # map the iterable to integer\n",
    "    d, m, y = map(int, date.split('/'))\n",
    "    # year is abbreviated so we need to add 20 in front\n",
    "    y = int(f\"20{y}\")\n",
    "    h, mins = map(int, time.split(':'))\n",
    "    return datetime(y, m, d, h, mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23558b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:55:49.349148Z",
     "start_time": "2021-07-04T02:55:47.978037Z"
    }
   },
   "outputs": [],
   "source": [
    "sdf.withColumn(\"tpep_pickup_datetime\", format_dtime('tpep_pickup_datetime')) \\\n",
    "    .withColumn(\"tpep_dropoff_datetime\", format_dtime('tpep_dropoff_datetime')) \\\n",
    "    .limit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889c4765",
   "metadata": {},
   "source": [
    "- Conversion looks good to me, so let's keep it.\n",
    "- Remember, Spark is immutable, so we will need to overwrite the `sdf` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce854550",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:57:37.132596Z",
     "start_time": "2021-07-04T02:57:37.113212Z"
    }
   },
   "outputs": [],
   "source": [
    "sdf = sdf.withColumn(\"tpep_pickup_datetime\", format_dtime('tpep_pickup_datetime')) \\\n",
    "    .withColumn(\"tpep_dropoff_datetime\", format_dtime('tpep_dropoff_datetime'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd2d96d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:57:38.138098Z",
     "start_time": "2021-07-04T02:57:37.294058Z"
    }
   },
   "outputs": [],
   "source": [
    "sdf.limit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6510b44",
   "metadata": {},
   "source": [
    "## Creating a SQL Table with an existing Spark DataFrame\n",
    "The easiest method is to use `sdf.createOrReplaceTempView(TABLE_NAME)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4deaeaf6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:57:47.665318Z",
     "start_time": "2021-07-04T02:57:47.637690Z"
    }
   },
   "outputs": [],
   "source": [
    "sdf.createOrReplaceTempView(\"taxi_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0dad22",
   "metadata": {},
   "source": [
    "Select all columns from our table, where:\n",
    "- the Vendor is `VeriFone Inc.`;\n",
    "- we have at least 1 passenger;\n",
    "- and a trip distance greater than 1 mile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bbd999",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:58:04.027320Z",
     "start_time": "2021-07-04T02:58:03.246894Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sql_query = \"\"\"\n",
    "SELECT * \n",
    "FROM taxi_data\n",
    "WHERE VendorID = 2\n",
    "    AND passenger_count >= 1\n",
    "    AND trip_distance >= 1\n",
    "LIMIT 5;\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(sql_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ef7fde",
   "metadata": {},
   "source": [
    "Below is the alternative query using PySpark. As you can see, it becomes _less interpretable_. Using Spark SQL ensures that the query is consistent and can also be run directly on the database if need be, allowing for a much more consistent way of testing queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dedbd63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T03:00:10.885647Z",
     "start_time": "2021-07-04T03:00:10.104033Z"
    }
   },
   "outputs": [],
   "source": [
    "sdf.select(sdf.columns) \\\n",
    "    .filter((col('VendorID') == 2) & (col('passenger_count') >= 1) & (col('trip_distance') >= 1)) \\\n",
    "    .limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6950cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T03:00:12.608666Z",
     "start_time": "2021-07-04T03:00:12.574764Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0b08f2",
   "metadata": {},
   "source": [
    "## Creating SQL views directly from files\n",
    "- If you don't have a Spark DataFrame, you can still read it in directly using Spark SQL.\n",
    "\n",
    "Syntax:\n",
    "```python\n",
    "q = \"\"\"\n",
    "CREATE OR REPLACE TEMPORARY VIEW PARQUET_NAME\n",
    "USING parquet\n",
    "OPTIONS (path PARQUET_FPATH)\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "- `CREATE OR REPLACE TEMPORARY VIEW TABLE_NAME` is the same as `sdf.createOrReplaceTempView(TABLE_NAME)`.\n",
    "- `USING` denotes the file type (i.e `csv`).\n",
    "- `OPTIONS` indicates the file path we wish to read from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cf7e22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T03:00:16.234501Z",
     "start_time": "2021-07-04T03:00:15.884283Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "sql_query = f\"\"\"\n",
    "CREATE OR REPLACE TEMPORARY VIEW aggregation_parquet\n",
    "USING parquet\n",
    "OPTIONS (path \n",
    "    \"{'/'.join(os.getcwd().split('/')[:-1])}/data/aggregated_results.parquet/\")\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e1b0ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T03:00:18.381877Z",
     "start_time": "2021-07-04T03:00:17.997769Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM aggregation_parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
